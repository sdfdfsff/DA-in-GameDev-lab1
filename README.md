# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #1 выполнил(а):
- Артеменко Антон Сергеевич
- РИ210950
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 40 |
| Задание 2 | * | 100 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Ознакомиться с интеграцией экономической системы в проект Unity и обучением ML-Agent

## Задание 1
### Измените параметры файла .yaml-агента и определить какие параметры и как влияют на обучение модели

Написать программу на python и unity, которая выводит Hello World

1. learnin_rate - Начальная скорость обучения для градиентного спуска. Его следует уменьшить чтобы тренеровка была стабильной.

2. beta - Сила регуляризации энтропии. beta-значение должно быть таким, чтобы энтропия медленно уменьшалась вместе с увеличением вознаграждения.

3. epsilon - Соответствует допустимому порогу расхождения между старой и новой политиками обучения при обновлении с градиентным спуском. Установка этого значения небольшим приведет к более стабильным обновлениям, но также замедлит процесс обучения.

4. num_epochs - Количество проходов, которые необходимо выполнить через буфер опыта при выполнении оптимизации "градиентного спуска". Уменьшение этого параметра обеспечит более стабильные обновления за счет более медленного обучения.

5. save_steps - Количество шагов тренажера между снапшотами текущей политики обучения.

## Задание 2
### Опишите результаты, выведенные в TensorBoard
![Image alt](https://github.com/sdfdfsff/DA-in-GameDev-lab1/blob/lab5/Screenshot_6.png)
1. вознаграждения по идее должен постепенно возрастать, при выставлении параметров для постепенности вознаграждения и если вознаграждение сразу не упрется в единицу
![Image alt](https://github.com/sdfdfsff/DA-in-GameDev-lab1/blob/lab5/Screenshot_4.png)
3. постепенно уменьшающееся значение потерь что и логично в при обучении модели
![Image alt](https://github.com/sdfdfsff/DA-in-GameDev-lab1/blob/lab5/Screenshot_5.png)
5. также постепенное уменьшение наблюдается так как по мере обучения модель должна принимать все более устаканенные значения

## Выводы

- В данной работе мы внедрили экономическую систему и обучение ML-агента в преокт Unity, познакомились с TensorBoard и файлом конфигурации ML-агента, наглядно увидели как влияют поля конфигурации на обучение агента
